{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Record Data Analysis with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## [Trip Record Data](https://www.nyc.gov/site/tlc/about/). \n",
    "\n",
    "`Yellow` and `green` taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
    "\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">I am assuming that you have installed apache spark and java tools in order to run everything.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk-11'  # Path to Java\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark-3.4.3-bin-hadoop3'  # Path to spark\n",
    "# os.environ['HADOOP_HOME'] = 'C:\\hadoop'\n",
    "# os.environ['PATH'] += r'C:\\hadoop\\bin'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python'\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.spark:spark-sql_2.12:3.5.3\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow Records\n",
    "- **VendorID** : A code indicating the TPEP provider that provided the record.\n",
    "- **tpep_pickup_datetime** : The date and time when the meter was engaged.\n",
    "- **tpep_dropoff_datetime** : The date and time when the meter was disengaged.\n",
    "- **Passenger_count**: The number of passengers in the vehicle. This is a driver-entered value.\n",
    "- **Trip_distance**: The elapsed trip distance in miles reported by the taximeter.\n",
    "- **PULocationID**: TLC Taxi Zone in which the taximeter was engaged.\n",
    "- **DOLocationID**: TLC Taxi Zone in which the taximeter was disengaged.\n",
    "- **RateCodeID**: The final rate code in effect at the end of the trip. 1= Standard rate  2=JFK 3=Newark 4=Nassau or Westchester 5=Negotiated fare 6=Group ride\n",
    "- **Store_and_fwd_flag**: This flag indicates whether the trip record was held in vehicle  memory before sending to the vendor, aka “store and forward,”  because the vehicle did not have a connection to the server. Y= store and forward trip N= not a store and forward trip.\n",
    "- **Payment_type**: A numeric code signifying how the passenger paid for the trip.  1= Credit card 2= Cash 3= No charge 4= Dispute 5= Unknown 6= Voided trip\n",
    "- **Fare_amount**: The time-and-distance fare calculated by the meter.\n",
    "- **Extra**: Miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges.\n",
    "- **MTA_tax**: $0.50 MTA tax that is automatically triggered based on the metered  rate in use.\n",
    "- **Improvement_surcharge**: $0.30 improvement surcharge assessed trips at the flag drop. The  improvement surcharge began being levied in 2015.\n",
    "- **Tip_amount**: Tip amount – This field is automatically populated for credit card  tips. Cash tips are not included.\n",
    "- **Tolls_amount**: Total amount of all tolls paid in trip. \n",
    "- **Total_amount**: The total amount charged to passengers. Does not include cash tips.\n",
    "- **Congestion_Surcharge**: Total amount collected in trip for NYS congestion surcharge.\n",
    "- **Airport_fee**: $1.25 for pick up only at LaGuardia and John F. Kennedy Airports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DecimalType, StringType, TimestampType, DoubleType, DateType, TimestampNTZType, LongType\n",
    "\n",
    "schema = StructType([StructField('VendorID', IntegerType(), True), \n",
    "            StructField('tpep_pickup_datetime', TimestampNTZType(), True), \n",
    "            StructField('tpep_dropoff_datetime', TimestampNTZType(), True), \n",
    "            StructField('passenger_count', LongType(), True), \n",
    "            StructField('trip_distance', DoubleType(), True), \n",
    "            StructField('RatecodeID', LongType(), True), \n",
    "            StructField('store_and_fwd_flag', StringType(), True), \n",
    "            StructField('PULocationID', IntegerType(), True), \n",
    "            StructField('DOLocationID', IntegerType(), True), \n",
    "            StructField('payment_type', LongType(), True), \n",
    "            StructField('fare_amount', DoubleType(), True), \n",
    "            StructField('extra', DoubleType(), True), \n",
    "            StructField('mta_tax', DoubleType(), True), \n",
    "            StructField('tip_amount', DoubleType(), True), \n",
    "            StructField('tolls_amount', DoubleType(), True), \n",
    "            StructField('improvement_surcharge', DoubleType(), True), \n",
    "            StructField('total_amount', DoubleType(), True), \n",
    "            StructField('congestion_surcharge', DoubleType(), True), \n",
    "            StructField('Airport_fee', DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 32.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Path of each file in order to read and union every parquet. We are using only Yellow trips\n",
    "files = [r'D:\\github\\PySpark\\Databases\\yellow_tripdata_2024-01.parquet', \n",
    "         r'D:\\github\\PySpark\\Databases\\yellow_tripdata_2024-02.parquet', \n",
    "         r'D:\\github\\PySpark\\Databases\\yellow_tripdata_2024-03.parquet']\n",
    "\n",
    "# Create a DataFrame empty with the schema that we defined before.\n",
    "df = spark.createDataFrame([], schema)\n",
    "\n",
    "for file in tqdm(files):\n",
    "    temp = spark.read.format(\"parquet\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .schema(schema) \\\n",
    "            .load(file)\n",
    "    \n",
    "    df = df.union(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has a total of 9554778 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f'DataFrame has a total of {df.count()} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new Columns\n",
    "We'd like to add day of the week, hour, duration of the trip. More information about Spark SQL functions [pypsary.sql.functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the week\n",
    "\n",
    "# Notación Punto\n",
    "from pyspark.sql.functions import dayofweek, hour, unix_timestamp, col, round\n",
    "\n",
    "# Extract the day of the week of a given date/timestamp as integer. \n",
    "# Ranges from 1 for a Sunday through to 7 for a Saturday\n",
    "df = df.withColumn(\"DAY_OF_WEEK\", dayofweek(df[\"tpep_pickup_datetime\"]))\n",
    "\n",
    "# Get hour of a given date\n",
    "df = df.withColumn(\"START_HOUR\", hour(df[\"tpep_pickup_datetime\"]))\n",
    "\n",
    "# Calculate difference between tpep_dropoff_datetime and tpep_pickup_datetime\n",
    "df = df.withColumn(\n",
    "    \"time_diff_minutes\",\n",
    "   round( (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 60, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = list(np.linspace(0,10000,10001))\n",
    "\n",
    "# Usar Bucketizer para asignar bins\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"time_diff_minutes\", outputCol=\"bin\")\n",
    "df_binned = bucketizer.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.565972222222222"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9455 / 60) / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min_value|max_value|\n",
      "+---------+---------+\n",
      "|   -52.07|   9455.4|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_binned.selectExpr(\"min(time_diff_minutes) as min_value\", \"max(time_diff_minutes) as max_value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Values for each bin\n",
    "df_binned.createOrReplaceTempView(\"bin_df\")\n",
    "\n",
    "query = \"\"\" \n",
    "    SELECT \n",
    "        bin,\n",
    "        COUNT(VendorID)\n",
    "    FROM\n",
    "        bin_df\n",
    "    WHERE\n",
    "        time_diff_minutes > 0\n",
    "    GROUP BY\n",
    "        bin\n",
    "    ORDER BY\n",
    "        bin\n",
    "\"\"\"\n",
    "\n",
    "histogram = spark.sql(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
